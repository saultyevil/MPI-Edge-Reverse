#include <stdlib.h>
#include <stdio.h>
#include <math.h>
#include <mpi.h>

#include "image_constants.h"
#include "image_functions.h"

int
main(int argc, char *argv[])
{
    int i, j, iter, recv_iter, n_iters, check_freq, out_freq, verbose;
    int nx, ny, nx_proc, ny_proc, proc, n_procs;
    double delta_stopping;
    char *in_filename = malloc(sizeof(*in_filename) * MAX_LINE);
    char *out_filename = malloc(sizeof(*out_filename) * MAX_LINE);

    MPI_Comm cart_comm;
    MPI_Status recv_status;
    MPI_Datatype send_array_vector, send_halo_vector;

    MPI_Init(NULL, NULL);
    MPI_Comm_rank(DEFAULT_COMM, &proc);
    MPI_Comm_size(DEFAULT_COMM, &n_procs);

    /*
     * Read in the parameters, filename etc, from an external config file, then
     * read in the image using pgmread provided by our friend David.
     */
    read_int("MAX_ITERS", &n_iters);
    read_int("CHECK_FREQ", &check_freq);
    read_int("OUTPUT_FREQ", &out_freq);
    read_int("VERBOSE", &verbose);
    read_double("DELTA", &delta_stopping);
    read_string("INPUT_FILENAME", in_filename);
    read_string("OUTPUT_FILENAME", out_filename);

    pgmsize(in_filename, &nx, &ny);
    double **master_buff = arralloc(sizeof(*master_buff), 2, nx, ny);

    if (proc == MASTER_PROCESS)
    {
        pgmread(in_filename, &master_buff[0][0], nx, ny);
        printf("\n----------------------\n");
        printf("INPUT_FILENAME: %s\nOUTPUT_FILENAME: %s\nRESOLUTION: %d x %d\n",
               in_filename, out_filename, nx, ny);
        printf("MAX_ITERS: %d\nCHECK_FREQ: %d\nOUTPUT_FREQ: %d\nDELTA: %4.2f\n",
               n_iters, check_freq, out_freq, delta_stopping);
        printf("VERBOSE: %d\nNDIMS: %d\nN_PROCS: %d\n", verbose, NDIMS,
               n_procs);
        printf("----------------------\n");
    }

    free(in_filename);

    /*
     * Calculate the boundaries for each process.
     * dims is initialised as being 0 otherwise MPI_Dims_create will likely
     * return garbage. The directions are non-periodic. A cartesian toplogy is
     * created and the neighbouring processes for a process are found using
     * MPI_Cart_shift
     */
    int reorder = 0, disp = 1;
    int *nbrs = malloc(sizeof(*nbrs) * N_NBRS);
    int *dims = malloc(sizeof(*dims) * NDIMS);
    int *dim_period = malloc(sizeof(*dim_period) * NDIMS);
    int *coords = malloc(sizeof(*coords) * NDIMS);
    int *bounds = malloc(sizeof(*bounds) * N_NBRS);

    /*
     * If NDIMS is too large or small, the program will exit in this function
     */
    cart_comm = create_topology(dims, dim_period, nbrs, coords, nx, ny,
                                &nx_proc, &ny_proc, &proc, n_procs,
                                reorder, disp);

    /*
     * Allocate memory for all of the arrays -- use arralloc because it keeps
     * array elements contiguous
     */
    double **buff = arralloc(sizeof(*buff), 2, nx_proc, ny_proc);
    double **old = arralloc(sizeof(*old), 2, nx_proc+2, ny_proc+2);
    double **new = arralloc(sizeof(*new), 2, nx_proc+2, ny_proc+2);
    double **edge = arralloc(sizeof(*edge), 2, nx_proc+2, ny_proc+2);

    /*
     * Define a MPI vector types for sending sections of arrays
     * send_array_vector - this will send a 2D section of the array between
     *                     master buff to buff
     * send_halo_vector - this will send the halos of a specified dimenion
     */
    MPI_Type_vector(nx_proc, ny_proc, nx, MPI_DOUBLE, &send_array_vector);
    MPI_Type_commit(&send_array_vector);
    MPI_Type_vector(nx_proc, 1, ny_proc + 2, MPI_DOUBLE, &send_halo_vector);
    MPI_Type_commit(&send_halo_vector);

    if (verbose == TRUE)
        print_dims_coords(dims, coords, proc, n_procs, cart_comm);

    /*
     * Send all of the coordinates to the master process for use when
     * distributing master_buff over buff in all the processes
     */
    int *x_dim_coords = malloc(sizeof(*x_dim_coords) * n_procs);
    int *y_dim_coords = malloc(sizeof(*y_dim_coords) * n_procs);

    MPI_Gather(&coords[0], 1, MPI_INT, x_dim_coords, 1, MPI_INT,
               MASTER_PROCESS, cart_comm);
    MPI_Gather(&coords[1], 1, MPI_INT, y_dim_coords, 1, MPI_INT,
               MASTER_PROCESS, cart_comm);

    double parallel_begin = MPI_Wtime();

    /*
     * Distribute masterbuff into smaller buffs for each process running
     */
    scatter_buffer(master_buff, buff, x_dim_coords, y_dim_coords,
                   nx_proc, ny_proc, proc, n_procs, send_array_vector,
                   cart_comm);

    /*
     * Copy the buffer array (containing the image) to the edge array and
     * initialise the output image as being completely white -- this acts as
     * an initial guess and sets up halo boundary conditions
     */
    init_arrays(edge, old, buff, nx_proc, ny_proc);

    /*
     * Compute the iterations -- time them for parallel computation comparison
     */
    double parallel_iters_start = MPI_Wtime();

    compute_iterations(old, new, edge, nbrs, nx_proc, ny_proc, proc, n_iters,
        delta_stopping, check_freq, out_freq, verbose, send_halo_vector,
        cart_comm);

    double parallel_iters_end = MPI_Wtime();

    /*
     * Copy the final image into the buffer -- excluding halo cells i.e.
     * copy old into buff
     */
    copy_to_buff(old, buff, nx_proc, ny_proc);

    free(x_dim_coords); free(y_dim_coords); free(dims); free(dim_period);
    free(coords); free(bounds); free(nbrs); free(old); free(new); free(edge);

    /*
     * Gather all the buff's back into the masterbuff on the master process
     */
    gather_buffer(master_buff, buff, x_dim_coords, y_dim_coords,
                  nx_proc, ny_proc, proc, n_procs, send_array_vector,
                  cart_comm);

    double parallel_end = MPI_Wtime();

    MPI_Finalize();

    free(buff);

    if (proc == MASTER_PROCESS)
    {
        pgmwrite(out_filename, &master_buff[0][0], nx, ny);
        printf("Time required for image conversion: %9.6f seconds.\n\n",
            (parallel_iters_end - parallel_iters_start));
    }

    free(master_buff);
    free(out_filename);

    return 0;
}
